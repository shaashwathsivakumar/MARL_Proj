{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from pettingzoo.mpe import simple_tag_v2\n",
        "import supersuit as ss"
      ],
      "metadata": {
        "id": "ozeh0k5y0TfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Actor(nn.Module):\n",
        "  def __init__(self, state_size, action_size):\n",
        "    super(Actor, self).__init__()\n",
        "    self.fc1 = nn.Linear(state_size, 64)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    # Give the desired size for the output layer\n",
        "    self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = torch.relu(self.fc1(torch.tensor(state)))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    # Obtain the action probabilities\n",
        "    action_probs = torch.softmax(self.fc3(x), dim=-1)\n",
        "    return action_probs\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self, state_size):\n",
        "    super(Critic, self).__init__()\n",
        "    self.fc1 = nn.Linear(state_size, 64)\n",
        "    # Fill in the desired dimensions\n",
        "    self.fc2 = nn.Linear(64, 1)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = torch.relu(self.fc1(torch.tensor(state)))\n",
        "    # Calculate the output value\n",
        "    value = self.fc2(x)\n",
        "    return value"
      ],
      "metadata": {
        "id": "9ejCvnQ89pPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen = capacity)\n",
        "\n",
        "  def push(self, obs, actions, rewards, next_obs, done):\n",
        "    data = (obs, actions, rewards, next_obs, done)\n",
        "    self.buffer.append(data)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ],
      "metadata": {
        "id": "jk6yN3RaKr2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DO ALL THE ENV SETUP HERE\n",
        "#init actor and critic\n",
        "#create target networks\n",
        "#\n",
        "\n",
        "for ep in range(episodes):\n",
        "  obs, info = env.reset()\n",
        "  dones = {agent: False for agent in env.agents}\n",
        "\n",
        "  while not all(dones.values()):\n",
        "    actions = {}\n",
        "    for i, agent in enumerate(env.agents):\n",
        "      state = obs[agent]\n",
        "      with torch.no_grad():\n",
        "          probs = actors[i](state) #computes prob distribution over actions\n",
        "          action = torch.multinomial(probs, num_samples=1).item() #samples one action from the distribution\n",
        "      actions[agent] = action\n",
        "\n",
        "    next_obs, rewards, dones_2, _, _ = env.step(actions)\n",
        "\n",
        "    # Store in replay buffer\n",
        "    replay_buffer.push(\n",
        "        [obs[agent] for agent in env.agents],\n",
        "        [actions[agent] for agent in env.agents],\n",
        "        [rewards[agent] for agent in env.agents],\n",
        "        [next_obs[agent] for agent in env.agents],\n",
        "        [dones[agent] for agent in env.agents]\n",
        "    )\n",
        "\n",
        "    obs = next_obs\n",
        "    dones = dones_2\n",
        "\n",
        "    for i in range(n_agents):\n",
        "      obs_s, act_s, rew_s, next_obs_s, done_s = replay_buffer.sample(batch_size)\n",
        "      #convert to lists of tensors\n",
        "      obs_cat = torch.cat(obs_s, dim=1).float().to(device)\n",
        "      act_cat = torch.cat(act_s, dim=1).float().to(device)\n",
        "      next_obs_cat = torch.cat(next_obs_s, dim=1).float().to(device)\n",
        "      with torch.no_grad():\n",
        "        next_actions = [target_actors[j](next_obs_s[j]) for j in range(n_agents)]\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRZSF_g0Lb31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Training Loop"
      ],
      "metadata": {
        "id": "qNuULCvN0VtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(maddpg: MADDPG, env, episodes=200):\n",
        "    for ep in range(episodes):\n",
        "        obs = env.reset()\n",
        "        state = env.state()\n",
        "        for a in maddpg.agents:\n",
        "            maddpg.noises[a].reset()\n",
        "\n",
        "        done = {a: False for a in maddpg.agents}\n",
        "        episode_reward = {a: 0.0 for a in maddpg.agents}\n",
        "\n",
        "        while not all(done.values()):\n",
        "\n",
        "            acts = {}\n",
        "            for a in maddpg.agents:\n",
        "                obs_tensor = torch.FloatTensor(obs[a]).unsqueeze(0)\n",
        "                action = maddpg.actors[a](obs_tensor).detach().numpy()[0]\n",
        "                action += maddpg.noises[a].sample()\n",
        "                acts[a] = np.clip(action, -1.0, 1.0)\n",
        "\n",
        "\n",
        "            next_obs, rews, done, infos = env.step(acts)\n",
        "            next_state = env.state()\n",
        "\n",
        "            maddpg.buffer.push(state, obs, acts, rews, next_state, next_obs, done)\n",
        "\n",
        "            obs, state = next_obs, next_state\n",
        "\n",
        "            for a in maddpg.agents:\n",
        "                episode_reward[a] += rews[a]\n",
        "\n",
        "            maddpg.update()\n",
        "\n",
        "        print(f\"Episode {ep:3d} rewards:\", episode_reward)\n",
        "\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "5VDincgAzxLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.distributions import Categorical\n",
        "from pettingzoo.mpe import simple_tag_v2\n",
        "import supersuit as ss\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Replay Buffer\n",
        "# ----------------------------\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=int(1e6)):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def push(self, state, obs, actions, rewards, next_state, next_obs, dones):\n",
        "        \"\"\"Store a full multi-agent transition.\"\"\"\n",
        "        self.buffer.append((state, obs, actions, rewards, next_state, next_obs, dones))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        return map(list, zip(*batch))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Actor & Critic for Discrete Actions\n",
        "# ----------------------------\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, act_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        logits = self.net(obs)  # (batch, act_dim)\n",
        "        return F.softmax(logits, dim=-1)  # probabilities\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, total_act_dim, hidden_size=256):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim + total_act_dim, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size), nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state, joint_actions_onehot):\n",
        "        x = torch.cat([state, joint_actions_onehot], dim=-1)\n",
        "        return self.net(x)  # (batch, 1)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 3) MADDPG for Discrete Actions\n",
        "# ----------------------------\n",
        "class MADDPGDiscrete:\n",
        "    def __init__(self, env, gamma=0.95, tau=0.01,\n",
        "                 actor_lr=1e-3, critic_lr=1e-3,\n",
        "                 batch_size=1024, buffer_size=int(1e6)):\n",
        "        self.env = env\n",
        "        self.agents = env.possible_agents\n",
        "        self.n_agents = len(self.agents)\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Dimensions\n",
        "        example_obs = env.reset()\n",
        "        state_dim = env.state().shape[0]\n",
        "        obs_dims = {a: example_obs[a].shape[0] for a in self.agents}\n",
        "        act_dims = {a: env.action_spaces[a].n for a in self.agents}\n",
        "        total_act_dim = sum(act_dims.values())\n",
        "\n",
        "        # Networks & targets\n",
        "        self.actors = {}\n",
        "        self.targ_actors = {}\n",
        "        self.actor_optim = {}\n",
        "        for a in self.agents:\n",
        "            self.actors[a] = Actor(obs_dims[a], act_dims[a])\n",
        "            self.targ_actors[a] = Actor(obs_dims[a], act_dims[a])\n",
        "            self.targ_actors[a].load_state_dict(self.actors[a].state_dict())\n",
        "            self.actor_optim[a] = Adam(self.actors[a].parameters(), lr=actor_lr)\n",
        "\n",
        "        self.critic = Critic(state_dim, total_act_dim)\n",
        "        self.targ_critic = Critic(state_dim, total_act_dim)\n",
        "        self.targ_critic.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optim = Adam(self.critic.parameters(), lr=critic_lr)\n",
        "\n",
        "        # Replay buffer\n",
        "        self.buffer = ReplayBuffer(buffer_size)\n",
        "\n",
        "    def _onehot_actions(self, actions):\n",
        "        \"\"\"Convert dict of discrete actions to a joint one-hot tensor.\"\"\"\n",
        "        onehots = []\n",
        "        for a in self.agents:\n",
        "            batch_actions = actions[a]  # torch tensor (batch,)\n",
        "            onehots.append(F.one_hot(batch_actions, num_classes=self.env.action_spaces[a].n).float())\n",
        "        return torch.cat(onehots, dim=-1)  # (batch, total_act_dim)\n",
        "\n",
        "    def select_action(self, obs):\n",
        "        \"\"\"Given a single observation dict, select discrete actions.\"\"\"\n",
        "        actions = {}\n",
        "        for a in self.agents:\n",
        "            o = torch.FloatTensor(obs[a]).unsqueeze(0)  # (1, obs_dim)\n",
        "            probs = self.actors[a](o).detach().squeeze(0).numpy()  # (act_dim,)\n",
        "            m = Categorical(probs)\n",
        "            actions[a] = m.sample().item()\n",
        "        return actions\n",
        "\n",
        "    def update(self):\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample batch\n",
        "        states, obs_b, acts_b, rews_b, next_states, next_obs_b, dones_b = \\\n",
        "            self.buffer.sample(self.batch_size)\n",
        "        states = torch.FloatTensor(states)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "\n",
        "        # Build per-agent tensors\n",
        "        obs_tensor = {a: torch.FloatTensor(np.vstack([ob[a] for ob in obs_b])) for a in self.agents}\n",
        "        action_tensor = {a: torch.LongTensor([ac[a] for ac in acts_b]) for a in self.agents}\n",
        "        rew_tensor = {a: torch.FloatTensor([[r[a]] for r in rews_b]) for a in self.agents}\n",
        "        done_tensor = {a: torch.FloatTensor([[d[a]] for d in dones_b]) for a in self.agents}\n",
        "        next_obs_tensor = {a: torch.FloatTensor(np.vstack([no[a] for no in next_obs_b])) for a in self.agents}\n",
        "\n",
        "        # One-hot joint actions\n",
        "        all_actions_onehot = self._onehot_actions(action_tensor)\n",
        "\n",
        "        # Next actions from target actors => one-hot\n",
        "        next_action_probs = {a: self.targ_actors[a](next_obs_tensor[a]) for a in self.agents}\n",
        "        next_action_samples = {a: Categorical(next_action_probs[a]).sample() for a in self.agents}\n",
        "        next_actions_onehot = self._onehot_actions(next_action_samples)\n",
        "\n",
        "        # Critic update\n",
        "        with torch.no_grad():\n",
        "            target_q = {}\n",
        "            q_targ_vals = self.targ_critic(next_states, next_actions_onehot)\n",
        "            for a in self.agents:\n",
        "                target_q[a] = rew_tensor[a] + self.gamma * (1 - done_tensor[a]) * q_targ_vals\n",
        "\n",
        "        current_q = self.critic(states, all_actions_onehot)\n",
        "        critic_loss = 0\n",
        "        for a in self.agents:\n",
        "            critic_loss += F.mse_loss(current_q, target_q[a])\n",
        "        critic_loss /= self.n_agents\n",
        "\n",
        "        self.critic_optim.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optim.step()\n",
        "\n",
        "        # Actor update\n",
        "        for a in self.agents:\n",
        "            # re-sample actions for current obs\n",
        "            probs = self.actors[a](obs_tensor[a])\n",
        "            dist = Categorical(probs)\n",
        "            sampled = dist.sample()\n",
        "            log_prob = dist.log_prob(sampled).unsqueeze(1)  # (B,1)\n",
        "\n",
        "            # joint actions: replace agent a's actions with sampled\n",
        "            acts = {b: action_tensor[b] if b != a else sampled for b in self.agents}\n",
        "            joint_onehot = self._onehot_actions(acts)\n",
        "\n",
        "            # Actor loss: maximize Q => minimize -Q * log_prob\n",
        "            q_val = self.critic(states, joint_onehot)\n",
        "            actor_loss = -(log_prob * q_val).mean()\n",
        "\n",
        "            self.actor_optim[a].zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optim[a].step()\n",
        "\n",
        "        # Soft updates\n",
        "        for a in self.agents:\n",
        "            for p, target_p in zip(self.actors[a].parameters(),\n",
        "                                    self.targ_actors[a].parameters()):\n",
        "                target_p.data.copy_(self.tau * p.data + (1 - self.tau) * target_p.data)\n",
        "        for p, target_p in zip(self.critic.parameters(), self.targ_critic.parameters()):\n",
        "            target_p.data.copy_(self.tau * p.data + (1 - self.tau) * target_p.data)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Training Loop\n",
        "# ----------------------------\n",
        "def train(maddpg, env, episodes=500):\n",
        "    for ep in range(episodes):\n",
        "        obs = env.reset()\n",
        "        state = env.state()\n",
        "        done = {a: False for a in maddpg.agents}\n",
        "        ep_rewards = {a: 0.0 for a in maddpg.agents}\n",
        "\n",
        "        while not all(done.values()):\n",
        "            actions = maddpg.select_action(obs)\n",
        "            next_obs, rewards, done, _ = env.step(actions)\n",
        "            next_state = env.state()\n",
        "\n",
        "            # store transition\n",
        "            maddpg.buffer.push(state, obs, actions, rewards, next_state, next_obs, done)\n",
        "\n",
        "            obs, state = next_obs, next_state\n",
        "            for a in maddpg.agents:\n",
        "                ep_rewards[a] += rewards[a]\n",
        "\n",
        "            # update networks\n",
        "            maddpg.update()\n",
        "\n",
        "        print(f\"Episode {ep} rewards:\", ep_rewards)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_env = simple_tag_v2.env(max_cycles=100, continuous_actions=False)\n",
        "    env = ss.pad_observations_v0(base_env)\n",
        "    maddpg = MADDPGDiscrete(env)\n",
        "    train(maddpg, env)\n"
      ],
      "metadata": {
        "id": "tw503dfT0SyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AcjnyA1HD33x"
      }
    }
  ]
}